{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summaries:\n",
      "Input 1: The quick brown fox jumps over the lazy dog.\n",
      "Generated Summary: The quick brown fox jumps over the lazy dog. The quick brownfox jumps over a lazy dog in a bid to get the attention of the dog's owner, who is on the other side of the fence.\n",
      "Reference Summary: A fox jumps over a dog.\n",
      "\n",
      "Input 2: Artificial intelligence is a field of computer science focused on creating intelligent machines.\n",
      "Generated Summary: Artificial intelligence is a field of computer science focused on creating intelligent machines. It is the study of computer technology that can be applied to the creation of intelligent machines, including computers, robots, and other machines.\n",
      "Reference Summary: AI is about creating smart machines.\n",
      "\n",
      "ROUGE Scores:\n",
      "Average ROUGE-1: 0.2078\n",
      "Average ROUGE-2: 0.0714\n",
      "Average ROUGE-L: 0.1851\n",
      "\n",
      "Flesch-Kincaid Grade Level (FKGL):\n",
      "Average FKGL: 9.25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import textstat\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('models/billsum')\n",
    "tokenizer = BartTokenizer.from_pretrained('models/billsum')\n",
    "\n",
    "# Sample input texts (replace with your actual texts)\n",
    "input_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is a field of computer science focused on creating intelligent machines.\"\n",
    "]\n",
    "\n",
    "# Reference summaries for ROUGE comparison\n",
    "reference_summaries = [\n",
    "    \"A fox jumps over a dog.\",\n",
    "    \"AI is about creating smart machines.\"\n",
    "]\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "generated_summaries = []\n",
    "\n",
    "# Generate summaries\n",
    "for input_text in input_texts:\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=150,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4\n",
    "    )\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    generated_summaries.append(generated_summary)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "for generated_summary, reference_summary in zip(generated_summaries, reference_summaries):\n",
    "    scores = scorer.score(reference_summary, generated_summary)\n",
    "    rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "    rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "    rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# Average ROUGE scores\n",
    "avg_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1'])\n",
    "avg_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2'])\n",
    "avg_rougeL = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL'])\n",
    "\n",
    "# Calculate FKGL scores\n",
    "fkgl_scores = [textstat.flesch_kincaid_grade(summary) for summary in generated_summaries]\n",
    "average_fkgl = sum(fkgl_scores) / len(fkgl_scores)\n",
    "\n",
    "# Display results\n",
    "print(\"Generated Summaries:\")\n",
    "for i, summary in enumerate(generated_summaries):\n",
    "    print(f\"Input {i+1}: {input_texts[i]}\")\n",
    "    print(f\"Generated Summary: {summary}\")\n",
    "    print(f\"Reference Summary: {reference_summaries[i]}\\n\")\n",
    "\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\n",
    "print(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\n",
    "print(f\"Average ROUGE-L: {avg_rougeL:.4f}\")\n",
    "\n",
    "print(\"\\nFlesch-Kincaid Grade Level (FKGL):\")\n",
    "print(f\"Average FKGL: {average_fkgl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b021e98b4748d3a4e9202014ad34ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tldr-train.jsonl:   0%|          | 0.00/4.76M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c9f4fcacab4f05a6d3a93283fe60d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tldr-dev.jsonl:   0%|          | 0.00/1.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1e66a1aa9749589efc928b947bc4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tldr-test.jsonl:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63011963e124b269c0cced1544710f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05255ea5ac241a190e9447a97738ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74027d8250854fb48739a2e041bdc903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question_id', 'nl', 'cmd', 'oracle_man', 'cmd_name', 'tldr_cmd_name', 'manual_exist', 'matching_info'],\n",
      "        num_rows: 6414\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question_id', 'nl', 'cmd', 'oracle_man', 'cmd_name', 'tldr_cmd_name', 'manual_exist', 'matching_info'],\n",
      "        num_rows: 928\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question_id', 'nl', 'cmd', 'oracle_man', 'cmd_name', 'tldr_cmd_name', 'manual_exist', 'matching_info'],\n",
      "        num_rows: 1845\n",
      "    })\n",
      "})\n",
      "{'question_id': '0', 'nl': 'get the label of a fat32 partition', 'cmd': 'fatlabel {{/dev/sda1}}', 'oracle_man': ['fatlabel_3'], 'cmd_name': 'fatlabel', 'tldr_cmd_name': 'fatlabel', 'manual_exist': True, 'matching_info': {'token': ['|main|'], 'oracle_man': [['fatlabel_3', 'fatlabel_4']]}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the TLDR dataset with remote code execution allowed\n",
    "tldr = load_dataset('neulab/tldr', trust_remote_code=True)\n",
    "\n",
    "# Access the dataset splits\n",
    "tldr_train = tldr['train']\n",
    "tldr_validation = tldr['validation']  # Check if 'validation' is available\n",
    "tldr_test = tldr['test']\n",
    "\n",
    "# Inspect the dataset structure (optional)\n",
    "print(tldr)\n",
    "print(tldr_train[0])  # Print the first example from the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = tldr['test'].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original NL Description: delete a shared memory segment by id\n",
      "\n",
      "Generated Summary (Command): delete a shared memory segment by id by id or by id in the following manner: (i.e. delete a memory segment from the shared memory of a person who has shared a memory with another person): (1) delete a segment from a memory that is\n",
      "\n",
      "Reference Summary (Command): ipcrm --shmem-id {{shmem_id}}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Original NL Description: delete a shared memory segment by key\n",
      "\n",
      "Generated Summary (Command): delete a shared memory segment by key key by key by deleting a key from a key in a key memory segment and replacing it with a new key by a different key in the same key. delete a key and replace it with another key by another key.delete a\n",
      "\n",
      "Reference Summary (Command): ipcrm --shmem-key {{shmem_key}}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Original NL Description: delete an ipc queue by id\n",
      "\n",
      "Generated Summary (Command): delete an ipc queue by id by id to avoid the use of the id of an IPc queue on a computer with an id that is not the same as the id used to create the queue by the user's IP address.delete an IPC queue by\n",
      "\n",
      "Reference Summary (Command): ipcrm --queue-id {{ipc_queue_id}}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Original NL Description: delete an ipc queue by key\n",
      "\n",
      "Generated Summary (Command): delete an ipc queue by key by key: (i.e. delete an IPc queue from an address by key or a key from a key to a key that is not a key) to an address that is a key or an address in a key\n",
      "\n",
      "Reference Summary (Command): ipcrm --queue-key {{ipc_queue_key}}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Original NL Description: delete a semaphore by id\n",
      "\n",
      "Generated Summary (Command): delete a semaphore by id by id or by id to avoid the use of semaphores by the user of a mobile phone or other mobile device in the U.S. or any other country in the world in which such a device is used.\n",
      "\n",
      "Reference Summary (Command): ipcrm --semaphore-id {{semaphore_id}}\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "generated_summaries = []\n",
    "reference_summaries = []\n",
    "\n",
    "# Define maximum lengths for source and target text\n",
    "MAX_SOURCE_LENGTH = 512  # Maximum length of input text (e.g., 'nl')\n",
    "MAX_TARGET_LENGTH = 64   # Maximum length of the generated summary (e.g., 'cmd')\n",
    "\n",
    "\n",
    "# Ensure the model and tokenizer are moved to the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loop through the TLDR test examples\n",
    "with torch.no_grad():\n",
    "    for example in test_examples:\n",
    "        # Use the 'nl' field as the input text\n",
    "        inputs = tokenizer(\n",
    "            example['nl'],  # Input: natural language description\n",
    "            max_length=MAX_SOURCE_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate summary\n",
    "        summary_ids = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            num_beams=4,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "\n",
    "        # Decode generated summary\n",
    "        generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        generated_summaries.append(generated_summary)\n",
    "\n",
    "        # Use the 'cmd' field as the reference summary\n",
    "        reference_summaries.append(example['cmd'])\n",
    "\n",
    "        # Print results\n",
    "        print(\"\\nOriginal NL Description:\", example['nl'])\n",
    "        print(\"\\nGenerated Summary (Command):\", generated_summary)\n",
    "        print(\"\\nReference Summary (Command):\", example['cmd'])\n",
    "        print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average ROUGE Scores:\n",
      "rouge1: 0.1368\n",
      "rouge2: 0.0155\n",
      "rougeL: 0.1288\n",
      "\n",
      "Average Flesch-Kincaid Grade Level (FKGL): 8.0600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'generated_summaries': ['delete a shared memory segment by id by id or by id in the following manner: (i.e. delete a memory segment from the shared memory of a person who has shared a memory with another person): (1) delete a segment from a memory that is',\n",
       "  'delete a shared memory segment by key key by key by deleting a key from a key in a key memory segment and replacing it with a new key by a different key in the same key. delete a key and replace it with another key by another key.delete a',\n",
       "  \"delete an ipc queue by id by id to avoid the use of the id of an IPc queue on a computer with an id that is not the same as the id used to create the queue by the user's IP address.delete an IPC queue by\",\n",
       "  'delete an ipc queue by key by key: (i.e. delete an IPc queue from an address by key or a key from a key to a key that is not a key) to an address that is a key or an address in a key',\n",
       "  'delete a semaphore by id by id or by id to avoid the use of semaphores by the user of a mobile phone or other mobile device in the U.S. or any other country in the world in which such a device is used.'],\n",
       " 'reference_summaries': ['ipcrm --shmem-id {{shmem_id}}',\n",
       "  'ipcrm --shmem-key {{shmem_key}}',\n",
       "  'ipcrm --queue-id {{ipc_queue_id}}',\n",
       "  'ipcrm --queue-key {{ipc_queue_key}}',\n",
       "  'ipcrm --semaphore-id {{semaphore_id}}'],\n",
       " 'rouge_scores': {'rouge1': 0.136797163620693,\n",
       "  'rouge2': 0.015547169811320755,\n",
       "  'rougeL': 0.12879716362069302},\n",
       " 'fkgl_scores': [9.7, 9.5, 6.6, 6.2, 8.3],\n",
       " 'avg_fkgl_score': 8.059999999999999}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import textstat  # Library for readability scores\n",
    "\n",
    "# Define the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Define the Flesch-Kincaid Grade Level (FKGL) scorer\n",
    "def calculate_fkgl(text):\n",
    "    return textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "# List to hold the ROUGE scores and FKGL scores for each example\n",
    "rouge_scores = []\n",
    "fkgl_scores = []\n",
    "\n",
    "# Loop through the generated and reference summaries\n",
    "for gen, ref in zip(generated_summaries, reference_summaries):\n",
    "    # ROUGE scoring\n",
    "    score = scorer.score(gen, ref)\n",
    "    rouge_scores.append(score)\n",
    "\n",
    "    # FKGL scoring for the generated summary\n",
    "    fkgl_score = calculate_fkgl(gen)\n",
    "    fkgl_scores.append(fkgl_score)\n",
    "\n",
    "# Compute the average ROUGE scores\n",
    "avg_rouge_scores = {\n",
    "    'rouge1': np.mean([score['rouge1'].fmeasure for score in rouge_scores]),\n",
    "    'rouge2': np.mean([score['rouge2'].fmeasure for score in rouge_scores]),\n",
    "    'rougeL': np.mean([score['rougeL'].fmeasure for score in rouge_scores])\n",
    "}\n",
    "\n",
    "# Compute the average FKGL score\n",
    "avg_fkgl_score = np.mean(fkgl_scores)\n",
    "\n",
    "# Print the average ROUGE scores and FKGL score\n",
    "print(\"\\nAverage ROUGE Scores:\")\n",
    "for metric, score in avg_rouge_scores.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage Flesch-Kincaid Grade Level (FKGL): {avg_fkgl_score:.4f}\")\n",
    "\n",
    "# Prepare the summary results\n",
    "summary_results = {\n",
    "    'generated_summaries': generated_summaries,\n",
    "    'reference_summaries': reference_summaries,\n",
    "    'rouge_scores': avg_rouge_scores,\n",
    "    'fkgl_scores': fkgl_scores,\n",
    "    'avg_fkgl_score': avg_fkgl_score\n",
    "}\n",
    "\n",
    "# Display or return the summary results\n",
    "summary_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
