{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krishan098/legaldocumentsimplifier/blob/main/legal_text_simplification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUi8cRLOo3uZ",
        "outputId": "11b98681-756e-4bde-a82b-0641c2a13f68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy9i2mFxpH4s",
        "outputId": "08746320-798b-46b0-9c84-88375239b352"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjZwdIf5o3bh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "133401d0ae294f4da45b688df1d65986",
            "6ccf329da5464ead85b0df88ccb47440",
            "e2b4832c28314cd4b7a8a92594688a66",
            "f9afe930e71a4f2787da72f86dd773cc",
            "1a66e607d7af4ce48fe59eb900e464bb",
            "d13e5d29aeee4f6c81e4f901b5261ca9"
          ]
        },
        "id": "rPsJBbMRpM62",
        "outputId": "6a4c2c35-a454-45b2-c56b-bab526134f08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "133401d0ae294f4da45b688df1d65986",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ccf329da5464ead85b0df88ccb47440",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2b4832c28314cd4b7a8a92594688a66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9afe930e71a4f2787da72f86dd773cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a66e607d7af4ce48fe59eb900e464bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d13e5d29aeee4f6c81e4f901b5261ca9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8q60JimpUU2"
      },
      "outputs": [],
      "source": [
        "def bart_model(input_text):\n",
        "    inputs = tokenizer(input_text, max_length=1024, return_tensors=\"pt\", truncation=True)\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=512, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV_cHHV1mxVh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def preprocess(text):\n",
        "    text = remove_citations(text)\n",
        "    text = split_long_sentences(text)\n",
        "    text = replace_legal_terms(text)\n",
        "    text = standardize_structure(text)\n",
        "    return text\n",
        "\n",
        "def postprocess(text):\n",
        "    text = fix_formatting(text)\n",
        "    text = ensure_consistency(text)\n",
        "    text = add_paragraph_breaks(text)\n",
        "    return text\n",
        "\n",
        "def remove_citations(text):\n",
        "    return re.sub(r'\\(\\d+\\s+[A-Za-z\\.]+\\s+\\d+\\)', '', text)\n",
        "\n",
        "def split_long_sentences(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        if len(words) > 50:\n",
        "            splits = []\n",
        "            current_split = []\n",
        "\n",
        "            for token in sentence.split():\n",
        "                current_split.append(token)\n",
        "                if token == ',' or token == ';':\n",
        "                    splits.append(' '.join(current_split))\n",
        "                    current_split = []\n",
        "\n",
        "            if current_split:\n",
        "                splits.append(' '.join(current_split))\n",
        "            processed_sentences.extend(splits)\n",
        "        else:\n",
        "            processed_sentences.append(sentence)\n",
        "\n",
        "    return ' '.join(processed_sentences)\n",
        "\n",
        "def replace_legal_terms(text):\n",
        "    legal_terms = {\n",
        "    'hereinafter': 'from now on',\n",
        "    'pursuant to': 'according to',\n",
        "    'whereas': 'since',\n",
        "    'notwithstanding': 'despite',\n",
        "    'forthwith': 'immediately',\n",
        "    'inter alia': 'among other things',\n",
        "    'ab initio': 'from the beginning',\n",
        "    'ipso facto': 'by that fact itself',\n",
        "    'mutatis mutandis': 'with the necessary changes',\n",
        "    'de facto': 'in fact',\n",
        "    'de jure': 'by law',\n",
        "    'quid pro quo': 'something for something',\n",
        "    'sub judice': 'under judicial consideration',\n",
        "    'prima facie': 'at first glance',\n",
        "    'pro rata': 'in proportion',\n",
        "    'ultra vires': 'beyond the powers',\n",
        "    'res judicata': 'a matter already judged',\n",
        "    'a fortiori': 'even more so',\n",
        "    'ex parte': 'by one party',\n",
        "    'actus reus': 'guilty act',\n",
        "    'mens rea': 'guilty mind',\n",
        "    'nolo contendere': 'no contest',\n",
        "    'stare decisis': 'to stand by decided cases',\n",
        "    'in loco parentis': 'in the place of a parent',\n",
        "    'per curiam': 'by the court',\n",
        "    'amicus curiae': 'friend of the court',\n",
        "    'sui generis': 'unique',\n",
        "    'caveat emptor': 'let the buyer beware',\n",
        "    'habeas corpus': 'you shall have the body',\n",
        "    'ex post facto': 'after the fact',\n",
        "    'in situ': 'in its original place',\n",
        "    'pari passu': 'on equal footing',\n",
        "    'lex loci': 'law of the place',\n",
        "    'contra proferentem': 'against the drafter',\n",
        "    'pro bono': 'for the public good',\n",
        "    'ad hoc': 'for this specific purpose',\n",
        "    'ex officio': 'by virtue of office',\n",
        "    'jus cogens': 'compelling law',\n",
        "    'locus standi': 'right to bring action',\n",
        "    'nullum crimen sine lege': 'no crime without law',\n",
        "}\n",
        "\n",
        "\n",
        "    for term, replacement in legal_terms.items():\n",
        "        text = re.sub(r'\\b' + term + r'\\b', replacement, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "def standardize_structure(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'Section \\d+\\.', lambda m: '\\n' + m.group(0) + '\\n', text)\n",
        "    return text\n",
        "\n",
        "def fix_formatting(text):\n",
        "    text = '. '.join(s.strip().capitalize() for s in text.split('. '))\n",
        "    text = re.sub(r'([.!?])\\s*([A-Za-z])', r'\\1 \\2', text)\n",
        "    return text\n",
        "\n",
        "def ensure_consistency(text):\n",
        "    return text\n",
        "\n",
        "def add_paragraph_breaks(text):\n",
        "    text = re.sub(r'([.!?])\\s+(?=[A-Z])', r'\\1\\n\\n', text)\n",
        "    return text\n",
        "\n",
        "def process_document(text, model):\n",
        "    simplified_text = preprocess(text)\n",
        "    model_output = bart_model(simplified_text)\n",
        "    final_text = postprocess(model_output)\n",
        "    return final_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNmFCU_KoV5U"
      },
      "outputs": [],
      "source": [
        "text = '''WHEREAS, the parties hereto agree to the terms and conditions set forth in this Agreement; and, pursuant to Section 12.3,\n",
        " all disputes arising hereunder shall be resolved through arbitration. NOTWITHSTANDING any provision to the contrary, the obligations\n",
        "  herein shall commence forthwith. HEREINAFTER, the terms shall be interpreted according to the laws of the State of California.\n",
        "   Section 14. This document also includes, inter alia, provisions for confidentiality and data protection.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_bxHOE8ot98",
        "outputId": "b3d46b3a-528c-405b-b396-e35e040f941f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Since, the parties hereto agree to the terms and conditions set forth in this agreement.\n",
            "\n",
            "All disputes arising hereunder shall be resolved through arbitration.\n",
            "\n",
            "From now on, the terms shall be interpreted according to the laws of the state of california.\n",
            "\n",
            "This document also includes, among other things, provisions for confidentiality and data protection.\n"
          ]
        }
      ],
      "source": [
        "res = process_document(text, model)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlov4yyK-Eer"
      },
      "source": [
        "###Evaluation of fine tunned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7JmvYULAcqK"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQnIZ3Nypq9i"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "def extract_zip(path):\n",
        "    output_dir = f\"./fine_tuned_model/{os.path.splitext(os.path.basename(path))[0]}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(path, 'r') as zip_r:\n",
        "        zip_r.extractall(output_dir)\n",
        "    print(f\"Extracted to {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6birlDm-4ek",
        "outputId": "e843411a-0e04-4d28-b0c8-368230145282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted to ./fine_tuned_model/models-20250112T165419Z-001\n"
          ]
        }
      ],
      "source": [
        "extract_zip(\"/content/models-20250112T165419Z-001.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUIczw29-9lv",
        "outputId": "fcbc9f8b-ca6f-40f3-b218-77ea47b3fa68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted to ./fine_tuned_model/results-20250112T165420Z-001 (1)\n"
          ]
        }
      ],
      "source": [
        "extract_zip(\"/content/results-20250112T165420Z-001 (1).zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sInSm2WQARpC"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuKXciDv_mwo"
      },
      "outputs": [],
      "source": [
        "#load this model\n",
        "def load_fined_model():\n",
        "  model = AutoModelForSeq2SeqGeneration.from_pretrained('./content/fine_tuned_model/models-20250112T165419Z-001')\n",
        "  tokenizer = AutoTokenizer.from_pretrained('./content/fine_tuned_model/models-20250112T165419Z-001')\n",
        "  model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPQLeCUjAk6e"
      },
      "outputs": [],
      "source": [
        "#rogue metrics\n",
        "def rogue_met():\n",
        "    rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    smoothing = SmoothingFunction().method1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDFC9sajC9os",
        "outputId": "69d8f158-bdc7-4bdf-d91b-1ad5b7eb00a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=2236ee5ebf28937595aeb97ddd1d08ab65f33a986b8b0c449607986091b830b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge-score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "ucESVYglBk66",
        "outputId": "dc8892be-ad0f-4f05-9717-3e7a0cc7d843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No test data found in results zip. Only inference will be available.\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./model_files.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2c18e9911351>\u001b[0m in \u001b[0;36m<cell line: 131>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-2c18e9911351>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     evaluator = ModelEvaluator(\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mmodel_zip_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/models-20250112T165419Z-001.zip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mresults_zip_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/results-20250112T165420Z-001 (1).zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-2c18e9911351>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_zip_path, results_zip_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_zip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_zip_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_zip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_zip_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-2c18e9911351>\u001b[0m in \u001b[0;36minitialize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model_files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model_files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3777\u001b[0m                     )\n\u001b[1;32m   3778\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3779\u001b[0;31m                     raise EnvironmentError(\n\u001b[0m\u001b[1;32m   3780\u001b[0m                         \u001b[0;34mf\"Error no file named {_add_variant(WEIGHTS_NAME, variant)}, {_add_variant(SAFE_WEIGHTS_NAME, variant)},\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3781\u001b[0m                         \u001b[0;34mf\" {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME + '.index'} or {FLAX_WEIGHTS_NAME} found in directory\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./model_files."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "import zipfile\n",
        "import json\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, model_zip_path, results_zip_path):\n",
        "        self.setup_files(model_zip_path, results_zip_path)\n",
        "        self.initialize_model()\n",
        "        self.initialize_metrics()\n",
        "\n",
        "    def setup_files(self, model_zip_path, results_zip_path):\n",
        "        with zipfile.ZipFile(model_zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall('./model_files')\n",
        "\n",
        "        with zipfile.ZipFile(results_zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall('./results')\n",
        "\n",
        "        try:\n",
        "            with open('./results/test_data.json', 'r') as f:\n",
        "                self.test_data = json.load(f)\n",
        "        except:\n",
        "            print(\"No test data found in results zip. Only inference will be available.\")\n",
        "            self.test_data = None\n",
        "\n",
        "    def initialize_model(self):\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained('./model_files')\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('./model_files')\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "        print(f\"Model loaded and running on {self.device}\")\n",
        "\n",
        "    def initialize_metrics(self):\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        self.smoothing = SmoothingFunction().method1\n",
        "\n",
        "    def generate_simplified_text(self, input_text, max_length=512):\n",
        "        inputs = self.tokenizer(input_text,\n",
        "                              max_length=max_length,\n",
        "                              truncation=True,\n",
        "                              padding='max_length',\n",
        "                              return_tensors='pt')\n",
        "\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                num_beams=4,\n",
        "                length_penalty=2.0,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    def calculate_metrics(self, reference, hypothesis):\n",
        "        bleu = sentence_bleu([reference.split()],\n",
        "                           hypothesis.split(),\n",
        "                           smoothing_function=self.smoothing)\n",
        "        rouge_scores = self.rouge_scorer.score(reference, hypothesis)\n",
        "\n",
        "        return {\n",
        "            'bleu': bleu,\n",
        "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
        "            'rouge2': rouge_scores['rouge2'].fmeasure,\n",
        "            'rougeL': rouge_scores['rougeL'].fmeasure\n",
        "        }\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        if not self.test_data:\n",
        "            print(\"No test data available for evaluation\")\n",
        "            return None\n",
        "\n",
        "        results = []\n",
        "        for item in tqdm(self.test_data, desc=\"Evaluating\"):\n",
        "            input_text = item['input']\n",
        "            reference = item['target']\n",
        "            prediction = self.generate_simplified_text(input_text)\n",
        "            metrics = self.calculate_metrics(reference, prediction)\n",
        "            results.append({\n",
        "                'input': input_text,\n",
        "                'reference': reference,\n",
        "                'prediction': prediction,\n",
        "                **metrics\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(results)\n",
        "        avg_metrics = {\n",
        "            'avg_bleu': df['bleu'].mean(),\n",
        "            'avg_rouge1': df['rouge1'].mean(),\n",
        "            'avg_rouge2': df['rouge2'].mean(),\n",
        "            'avg_rougeL': df['rougeL'].mean()\n",
        "        }\n",
        "\n",
        "        return df, avg_metrics\n",
        "\n",
        "    def save_evaluation_results(self, df, avg_metrics, output_path='evaluation_results'):\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "        df.to_csv(f'{output_path}/detailed_results.csv', index=False)\n",
        "        with open(f'{output_path}/average_metrics.json', 'w') as f:\n",
        "            json.dump(avg_metrics, f, indent=4)\n",
        "\n",
        "    def simplify_new_text(self, text):\n",
        "        return self.generate_simplified_text(text)\n",
        "\n",
        "def main():\n",
        "    evaluator = ModelEvaluator(\n",
        "        model_zip_path='/content/models-20250112T165419Z-001.zip',\n",
        "        results_zip_path='/content/results-20250112T165420Z-001 (1).zip'\n",
        "    )\n",
        "\n",
        "    results = evaluator.evaluate_model()\n",
        "    if results:\n",
        "        df, avg_metrics = results\n",
        "        evaluator.save_evaluation_results(df, avg_metrics)\n",
        "        print(\"\\nAverage Metrics:\", json.dumps(avg_metrics, indent=2))\n",
        "\n",
        "    text = '''WHEREAS, the parties hereto agree to the terms and conditions set forth in this Agreement; and, pursuant to Section 12.3,\n",
        " all disputes arising hereunder shall be resolved through arbitration. NOTWITHSTANDING any provision to the contrary, the obligations\n",
        "  herein shall commence forthwith. HEREINAFTER, the terms shall be interpreted according to the laws of the State of California.\n",
        "   Section 14. This document also includes, inter alia, provisions for confidentiality and data protection.'''\n",
        "    simplified = evaluator.simplify_new_text(sample_text = text)\n",
        "    print(\"\\nSimplified text:\", simplified)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRlpTIPEECLE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}